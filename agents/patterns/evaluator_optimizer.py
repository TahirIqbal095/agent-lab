from env_setup import env_vars
from openai import AzureOpenAI
from google.genai import types
from google import genai
from pydantic import BaseModel, Field
import logging

logging.basicConfig(
    level= logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

logger = logging.getLogger(__name__) 

openai = AzureOpenAI(
    api_key=env_vars["api_key"],
    api_version=env_vars["api_version"],
    azure_endpoint=env_vars["azure_endpoint"],
)

google = genai.Client(api_key=env_vars["gemini_api_key"])


class OutputSchema(BaseModel):
    """
    Schema for the output generated by the hint generation model.
    """
    description: str = Field(
        ...,
        description="A concise summary of the problem statement and a high-level approach to solving it, without revealing the solution."
    )
    hints: str = Field(
        ...,
        description="A list containing up to 2 actionable hints to guide the user toward the solution, without giving away the answer."
    )

class EvaluatorSchema(BaseModel):
    """
    Schema for the evaluation of generated hints.
    """
    confidence_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="A score between 0 and 1 indicating confidence in the quality and helpfulness of the generated hints."
    )
    feedback: str = Field(
        ...,
        min_length=1,
        description="Constructive feedback on the hints, including suggestions for improvement if necessary."
    )

def generate_hint_with_gemini(input: str):
    try:
        logger.info("Generating hint for input: %s", input)
        response = google.models.generate_content(
            model="gemini-2.0-flash",
            config=types.GenerateContentConfig(
                system_instruction=(
                    "Analyze the LeetCode problem and generate hints to guide the user toward the solution. "
                    "Do not provide the solution. Give only two hints at most."
                ),
                response_schema=OutputSchema,
                response_mime_type="application/json",
            ),
            contents=input,
        )
        logger.info("Hint generation successful.")
        return response.text
    except Exception as e:
        logger.error("Failed to generate hint: %s", str(e))
        return None

def evaluator(hint: str, input: str):
    try:
        logger.info("Evaluating hint for input: %s", input)
        response = openai.beta.chat.completions.parse(
            model=env_vars["model"],
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are an evaluator that assesses hints generated by another LLM. "
                        "You will be given the hint and the problem statement. "
                        "Evaluate the hint and provide a confidence score (0-1) and constructive feedback for improvement. "
                        "Respond ONLY in valid JSON format."
                    ),
                },
                {"role": "user", "content": f"Problem Statement: {input}"},
                {"role": "user", "content": f"Hint: {hint}"},
            ],
            response_format=EvaluatorSchema,
        )
        logger.info("Evaluation successful.")
        content = response.choices[0].message.content
        return content
    except Exception as e:
        logger.error("Failed to evaluate hint: %s", str(e))
        return None
    

def main():
    input = "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice. You can return the answer in any order."
    
    result = generate_hint_with_gemini(input)
    if result is None:
        return

    try:
        output_data = OutputSchema.model_validate_json(result)
        hint = output_data.hints
    except Exception as e:
        logger.error("Failed to parse hint result: %s", str(e))
        return

    max_attempts = 5
    attempts = 0

    while attempts < max_attempts:
        eval_result = evaluator(hint, input)
        if eval_result is None:
            logger.error("Evaluation failed, aborting.")
            return

        try:
            eval_data = EvaluatorSchema.model_validate_json(eval_result)
        except Exception as e:
            logger.error("Failed to parse evaluation result: %s", str(e))
            return

        confidence = eval_data.confidence_score
        feedback = eval_data.feedback

        logger.info(f"Confidence: {confidence}, Feedback: {feedback}")

        if confidence >= 0.85:
            print("Final Hint:", hint)
            print("Evaluator Feedback:", feedback)
            break
        else:
            logger.info("Confidence below threshold, optimizing hint with feedback.")
            # Use feedback to generate a new hint
            prompt = (
                f"Problem Statement: {input}\n"
                f"Previous Hint: {hint}\n"
                f"Evaluator Feedback: {feedback}\n"
                "Please generate an improved hint based on the feedback above. "
                "Do not provide the solution. Give only two hints at most."
            )
            hint = generate_hint_with_gemini(prompt)
            if hint is None:
                logger.error("Failed to generate improved hint, aborting.")
                return
            attempts += 1
    else:
        print("Failed to generate a sufficiently confident hint after several attempts.")
        
        
        
if __name__ == "__main__":
    main()   